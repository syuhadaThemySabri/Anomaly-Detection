{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz27szCHCiAt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3l0T6_bC3hk"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('D:\\College\\Semester 6 (Magang)\\Model\\Anomaly Detection\\clean_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJCde3WSCnCi"
      },
      "outputs": [],
      "source": [
        "def preprocess_turbine_data(data):\n",
        "    \"\"\"\n",
        "    Preprocess the turbine data for anomaly detection\n",
        "    \"\"\"\n",
        "    # Convert timestamp and create time features\n",
        "    data['Timestamp'] = pd.to_datetime(data['Timestamp'], utc=True)\n",
        "\n",
        "    # Create time-based features\n",
        "    data['Hour'] = data['Timestamp'].dt.hour\n",
        "    data['Day'] = data['Timestamp'].dt.day\n",
        "    data['Month'] = data['Timestamp'].dt.month\n",
        "    data['Weekday'] = data['Timestamp'].dt.weekday\n",
        "\n",
        "    # Calculate rolling statistics for each feature\n",
        "    feature_columns = ['Ngp', 'Npt', 'HPC_ASV_Command', 'HPC_ASV_Position',\n",
        "                      'HPC_Surge_Margin', 'HPC_ASC_Flow_DP', 'HPC_Suction_Press',\n",
        "                      'HPC_Discharge_Press']\n",
        "\n",
        "    # Convert features to numeric\n",
        "    for col in feature_columns:\n",
        "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "    # Calculate rolling means and standard deviations\n",
        "    window_size = 24  # 24-hour window\n",
        "    for col in feature_columns:\n",
        "        data[f'{col}_rolling_mean'] = data[col].rolling(window=window_size).mean()\n",
        "        data[f'{col}_rolling_std'] = data[col].rolling(window=window_size).std()\n",
        "\n",
        "    return data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oYO02MTCpM_"
      },
      "outputs": [],
      "source": [
        "def detect_anomalies(data, contamination=0.02):\n",
        "    \"\"\"\n",
        "    Detect anomalies using Isolation Forest\n",
        "    \"\"\"\n",
        "    # Select features for anomaly detection\n",
        "    feature_cols = [col for col in data.columns\n",
        "                   if any(x in col for x in ['_rolling_mean', '_rolling_std'])]\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(data[feature_cols])\n",
        "\n",
        "    # Train Isolation Forest\n",
        "    iso_forest = IsolationForest(\n",
        "        n_estimators=100,\n",
        "        contamination=contamination,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Fit and predict\n",
        "    data['anomaly'] = iso_forest.fit_predict(scaled_features)\n",
        "    data['anomaly_score'] = iso_forest.score_samples(scaled_features)\n",
        "\n",
        "    return data, iso_forest, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42_aCyolCrv-"
      },
      "outputs": [],
      "source": [
        "def plot_anomalies(data):\n",
        "    \"\"\"\n",
        "    Create visualizations for the anomaly detection results\n",
        "    \"\"\"\n",
        "    # Create figure with subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
        "    fig.suptitle('Turbine Performance Anomaly Detection Results', fontsize=16)\n",
        "\n",
        "    # Plot 1: Time series with anomalies\n",
        "    ax1.plot(data['Timestamp'], data['HPC_ASC_Flow_DP'],\n",
        "             color='blue', label='HPC_ASC_Flow_DP', alpha=0.5)\n",
        "\n",
        "    # Highlight anomalies\n",
        "    anomalies = data[data['anomaly'] == -1]\n",
        "    ax1.scatter(anomalies['Timestamp'], anomalies['HPC_ASC_Flow_DP'],\n",
        "                color='red', label='Anomalies')\n",
        "\n",
        "    ax1.set_title('Time Series with Detected Anomalies')\n",
        "    ax1.set_xlabel('Timestamp')\n",
        "    ax1.set_ylabel('HPC_ASC_Flow_DP')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot 2: Anomaly scores distribution\n",
        "    sns.histplot(data=data, x='anomaly_score', bins=50, ax=ax2)\n",
        "    ax2.axvline(x=data[data['anomaly'] == -1]['anomaly_score'].max(),\n",
        "                color='red', linestyle='--', label='Anomaly Threshold')\n",
        "    ax2.set_title('Distribution of Anomaly Scores')\n",
        "    ax2.set_xlabel('Anomaly Score')\n",
        "    ax2.set_ylabel('Count')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCBKOvfvCtvN"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    data = pd.read_csv('D:\\College\\Semester 6 (Magang)\\Model\\Anomaly Detection\\clean_data.csv')  # Replace with your data path\n",
        "    processed_data = preprocess_turbine_data(data)\n",
        "\n",
        "    # Detect anomalies\n",
        "    results, model = detect_anomalies(processed_data)\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nAnomaly Detection Results:\")\n",
        "    print(f\"Total samples: {len(results)}\")\n",
        "    print(f\"Number of anomalies: {len(results[results['anomaly'] == -1])}\")\n",
        "    print(f\"Anomaly percentage: {(len(results[results['anomaly'] == -1]) / len(results)) * 100:.2f}%\")\n",
        "\n",
        "    # Create and show plots\n",
        "    fig = plot_anomalies(results)\n",
        "    plt.show()\n",
        "\n",
        "    return results, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvOeHBhuCwEH"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_anomalies(data):\n",
        "    \"\"\"\n",
        "    Create visualizations for the anomaly detection results, including a scatter plot\n",
        "    of anomaly scores vs. HPC_ASC_Flow_DP.\n",
        "    \"\"\"\n",
        "    # Create figure with subplots\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15))  # Added one more subplot\n",
        "    fig.suptitle('Turbine Performance Anomaly Detection Results', fontsize=16)\n",
        "\n",
        "    # Plot 1: Time series with anomalies\n",
        "    ax1.plot(data['Timestamp'], data['HPC_ASC_Flow_DP'],\n",
        "             color='blue', label='HPC_ASC_Flow_DP', alpha=0.5)\n",
        "\n",
        "    # Highlight anomalies\n",
        "    anomalies = data[data['anomaly'] == -1]\n",
        "    ax1.scatter(anomalies['Timestamp'], anomalies['HPC_ASC_Flow_DP'],\n",
        "                color='red', label='Anomalies')\n",
        "\n",
        "    ax1.set_title('Time Series with Detected Anomalies')\n",
        "    ax1.set_xlabel('Timestamp')\n",
        "    ax1.set_ylabel('HPC_ASC_Flow_DP')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot 2: Anomaly scores distribution with Gaussian curve\n",
        "    sns.histplot(data=data, x='anomaly_score', bins=50, ax=ax2, kde=False, stat='density', color='blue', label='Anomaly Scores')\n",
        "\n",
        "    # Fit a normal distribution to the data\n",
        "    mu, std = norm.fit(data['anomaly_score'])\n",
        "\n",
        "    # Plot the fitted Gaussian curve\n",
        "    xmin, xmax = ax2.get_xlim()\n",
        "    x = np.linspace(xmin, xmax, 100)\n",
        "    p = norm.pdf(x, mu, std)\n",
        "    ax2.plot(x, p, 'k', linewidth=2, label=f'Gaussian Fit (μ={mu:.2f}, σ={std:.2f})')\n",
        "\n",
        "    # Add vertical line for anomaly threshold\n",
        "    if not anomalies.empty: # Check if anomalies exist before accessing max\n",
        "        threshold = anomalies['anomaly_score'].max()\n",
        "        ax2.axvline(x=threshold, color='red', linestyle='--', label='Anomaly Threshold')\n",
        "    else:\n",
        "        threshold = None # Or handle it in a way that makes sense in your context\n",
        "\n",
        "    ax2.set_title('Distribution of Anomaly Scores with Gaussian Fit')\n",
        "    ax2.set_xlabel('Anomaly Score')\n",
        "    ax2.set_ylabel('Density')\n",
        "    ax2.legend()\n",
        "\n",
        "    # Plot 3: Scatter plot of Anomaly Score vs. HPC_ASC_Flow_DP\n",
        "    ax3.scatter(data['anomaly_score'], data['HPC_ASC_Flow_DP'], c=data['anomaly'].map({1: 'blue', -1: 'red'}), label='Data Points')\n",
        "    ax3.set_title('Anomaly Score vs. HPC_ASC_Flow_DP')\n",
        "    ax3.set_xlabel('Anomaly Score')\n",
        "    ax3.set_ylabel('HPC_ASC_Flow_DP')\n",
        "    if threshold is not None:\n",
        "        ax3.axvline(x=threshold, color='red', linestyle='--', label='Anomaly Threshold') # Add the threshold line here as well\n",
        "    ax3.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dq5bcQC0GyK3",
        "outputId": "7e9abb80-7d3c-487d-bb95-fd1df621568a"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    data = pd.read_csv('D:\\College\\Semester 6 (Magang)\\Model\\Anomaly Detection\\clean_data.csv')  \n",
        "    processed_data = preprocess_turbine_data(data)\n",
        "\n",
        "    # Detect anomalies\n",
        "    results, model, scaler = detect_anomalies(processed_data)\n",
        "\n",
        "    # Save the model and scaler to pkl files\n",
        "    model_filename = '.\\isolation_forest.pkl'\n",
        "    scaler_filename = '.\\isolation_forest_scaler.pkl'\n",
        "\n",
        "    with open(model_filename, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "    print(f\"\\nAnomaly detection model saved to: {model_filename}\")\n",
        "\n",
        "    with open(scaler_filename, 'wb') as file:\n",
        "        pickle.dump(scaler, file)\n",
        "    print(f\"Data scaler saved to: {scaler_filename}\")\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nAnomaly Detection Results:\")\n",
        "    print(f\"Total samples: {len(results)}\")\n",
        "    print(f\"Number of anomalies: {len(results[results['anomaly'] == -1])}\")\n",
        "    print(f\"Anomaly percentage: {(len(results[results['anomaly'] == -1]) / len(results)) * 100:.2f}%\")\n",
        "\n",
        "    # Create and show plots\n",
        "    fig = plot_anomalies(results)\n",
        "    plt.show()\n",
        "\n",
        "    return results, model, scaler\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, model, scaler = main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "anonmaly_detection_kernel",
      "language": "python",
      "name": "anonmaly_detection_kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
